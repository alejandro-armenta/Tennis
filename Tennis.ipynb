{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.36 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mjupyter-console 6.4.3 has requirement jupyter-client>=7.0.0, but you'll have jupyter-client 5.2.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from ddpg_agent import Agent, ReplayBuffer\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like:\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ] \n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.4669857  -1.5         0.          0.\n",
      " -6.83172083  6.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:\\n', states[0], \"\\n\", states[1])\n",
    "\n",
    "memory = ReplayBuffer(action_size, seed=10)\n",
    "\n",
    "agent = [Agent(state_size=state_size, action_size=action_size, memory_=memory, random_seed=10),\n",
    "         Agent(state_size=state_size, action_size=action_size, memory_=memory, random_seed=10)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 200\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 300\tAverage Score: 0.01\tScore: 0.10\n",
      "Episode 400\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 500\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 600\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 700\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 800\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 900\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 1000\tAverage Score: 0.00\tScore: 0.00\n",
      "Episode 1100\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 1200\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 1300\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 1400\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 1500\tAverage Score: 0.00\tScore: 0.00\n",
      "Episode 1600\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 1700\tAverage Score: 0.00\tScore: 0.00\n",
      "Episode 1800\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 1900\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 2000\tAverage Score: 0.00\tScore: 0.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f003c77a668>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I need to have the same actor to select actions\n",
    "# and the same replay buffer for both agents!\n",
    "\n",
    "def ddpg(n_episodes=2000, max_t=300):\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    total_scores = []\n",
    "    #max_score = -np.Inf\n",
    "    \n",
    "    noise = 2.0\n",
    "    noise_reduction = 0.9999\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        state_0 = env_info.vector_observations[0]\n",
    "        state_1 = env_info.vector_observations[1]\n",
    "        \n",
    "        #se resetea el noise aqu√≠!\n",
    "        agent[0].reset()\n",
    "        agent[1].reset()\n",
    "        \n",
    "        scores = np.zeros(num_agents)\n",
    "        \n",
    "        #for t in range(max_t):\n",
    "        while True:\n",
    "            \n",
    "            #print(states.shape)\n",
    "            \n",
    "            action_0 = agent[0].act(state_0, noise, True)\n",
    "            action_1 = agent[1].act(state_1, noise, True)\n",
    "            \n",
    "            action = [[action_0[0], action_0[1]], [action_1[0], action_1[1]]]\n",
    "            \n",
    "            #noise *= noise_reduction\n",
    "            \n",
    "            #print(action)\n",
    "            \n",
    "            #print(actions.shape)\n",
    "            \n",
    "            env_info = env.step(action)[brain_name]\n",
    "            \n",
    "            #print(\"env_info shape: \", env_info.shape)\n",
    "            \n",
    "            next_state_0 = env_info.vector_observations[0]\n",
    "            next_state_1 = env_info.vector_observations[1]\n",
    "            \n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            #print(dones)\n",
    "            \n",
    "            agent[0].step(state_0, action_0, env_info.rewards[0], next_state_0, dones[0])\n",
    "            agent[1].step(state_1, action_1, env_info.rewards[1], next_state_1, dones[1])\n",
    "            \n",
    "            \n",
    "            state_0 = next_state_0\n",
    "            state_1 = next_state_1\n",
    "            \n",
    "            scores += env_info.rewards\n",
    "            \n",
    "            if np.any(dones): \n",
    "                break\n",
    "                \n",
    "        \n",
    "                \n",
    "        # scores is a list with total rewards for episode!\n",
    "        # I need to get the max of the two agents' scores!\n",
    "        max_score = np.amax(scores)\n",
    "        scores_deque.append(max_score)\n",
    "        \n",
    "        #print(scores_deque)\n",
    "        \n",
    "        total_scores.append(max_score)\n",
    "        \n",
    "        #print(scores)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), max_score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            torch.save(agent[0].actor_local.state_dict(), 'checkpoint_actor_0.pth')\n",
    "            torch.save(agent[0].critic_local.state_dict(), 'checkpoint_critic_0.pth')\n",
    "            torch.save(agent[1].actor_local.state_dict(), 'checkpoint_actor_1.pth')\n",
    "            torch.save(agent[1].critic_local.state_dict(), 'checkpoint_critic_1.pth')\n",
    "            \n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n",
    "    return total_scores\n",
    "\n",
    "total_scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(total_scores)+1), total_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.22491062]\n",
      " [-0.68945266  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.43681276 -1.        ]\n",
      " [ 1.          1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.9048559  -0.43858773]\n",
      " [ 0.71358317 -0.09401837]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.56586621  1.        ]\n",
      " [ 0.01568866 -0.47135169]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          1.        ]\n",
      " [ 1.          0.27742683]]\n",
      "[0.0, 0.0]\n",
      "[[-0.42623293  0.9977718 ]\n",
      " [-0.42713641  0.26294999]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          0.98571382]\n",
      " [ 0.61224086 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.46934967 -1.        ]\n",
      " [ 0.67036675 -0.64941041]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.82991272  0.85881552]\n",
      " [-0.17701094 -0.13473652]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.2800029  -0.50908398]\n",
      " [ 1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.49447142  0.90869854]\n",
      " [ 1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.20707346]\n",
      " [-1.          1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.27690747 -1.        ]\n",
      " [-0.11862489 -0.07996661]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.62891132 -0.14547059]\n",
      " [ 0.36935128 -1.        ]]\n",
      "[-0.009999999776482582, 0.0]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "[[ 0.57954254  0.01052682]\n",
      " [-1.         -0.35268676]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.2544591 ]\n",
      " [-0.99290303  0.71269412]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -1.        ]\n",
      " [ 0.07510419 -0.55024687]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -0.93022099]\n",
      " [ 1.         -0.60515393]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          1.        ]\n",
      " [ 0.56162685 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.13982988 -0.49144928]\n",
      " [-1.         -0.86043635]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -0.76037645]\n",
      " [-1.          0.63176319]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          1.        ]\n",
      " [ 0.93664694  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.78278028 -1.        ]\n",
      " [ 0.44243455  0.89697611]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          0.93212294]\n",
      " [ 1.         -0.09774236]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          0.14446296]\n",
      " [ 0.63736465 -0.04804598]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.6868771   0.407648  ]\n",
      " [ 0.58736166 -0.71845274]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.33875309 -1.        ]\n",
      " [ 0.3199327   0.82687409]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.80426424 -0.29348722]\n",
      " [-0.29584809 -0.05012745]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.53062522  1.        ]\n",
      " [-1.          0.98792448]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.107277  1.      ]\n",
      " [-1.        1.      ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.26207307  1.        ]\n",
      " [-0.40547674  0.00106576]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          1.        ]\n",
      " [-0.97077907 -0.30216771]]\n",
      "[0.0, 0.10000000149011612]\n",
      "[[ 0.65261675 -0.23399648]\n",
      " [-0.70940385  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          0.51490605]\n",
      " [ 1.         -0.01332411]]\n",
      "[0.0, 0.0]\n",
      "[[-0.23290022  1.        ]\n",
      " [-1.         -0.34109144]]\n",
      "[0.0, 0.0]\n",
      "[[-0.64183583 -0.93233361]\n",
      " [ 1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.25419484 -0.9279918 ]\n",
      " [ 0.05614139 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.2513705   1.        ]\n",
      " [ 0.86491519  0.27223783]]\n",
      "[0.0, 0.0]\n",
      "[[-0.45592559  0.90135026]\n",
      " [-1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.8026211   0.55076228]\n",
      " [ 0.05150555  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.58632944 -0.61079825]\n",
      " [-1.          0.28873808]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -0.63144607]\n",
      " [ 0.35089595 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.57300188]\n",
      " [ 0.31475924  0.41502352]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.73019661]\n",
      " [-0.01418281 -0.59016491]]\n",
      "[0.0, 0.0]\n",
      "[[-0.77049808  0.01585254]\n",
      " [-0.59489129  1.        ]]\n",
      "[0.0, -0.009999999776482582]\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n",
      "[[ 1.         -0.31785685]\n",
      " [ 0.94100495 -0.92439571]]\n",
      "[0.0, 0.0]\n",
      "[[-0.53830633  0.89773066]\n",
      " [ 0.89946029  0.44853807]]\n",
      "[0.0, 0.0]\n",
      "[[-0.98842034 -0.8272441 ]\n",
      " [-1.          1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -1.        ]\n",
      " [-0.55946862 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.45548834 -0.8527352 ]\n",
      " [ 0.0446257  -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.02231937 -1.        ]\n",
      " [-1.         -0.52133203]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.84601311 -0.16668764]\n",
      " [-0.82630595 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.19304128 -0.3785096 ]\n",
      " [ 0.11387695 -0.56206629]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.87381765 -1.        ]\n",
      " [-0.26733118 -0.68832447]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          1.        ]\n",
      " [ 1.         -0.77476267]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -1.        ]\n",
      " [-0.78692939  0.89798685]]\n",
      "[0.0, 0.0]\n",
      "[[-0.55238295 -0.37127618]\n",
      " [-0.44944928 -0.06111971]]\n",
      "[0.0, 0.0]\n",
      "[[-0.62200809  0.6652522 ]\n",
      " [ 0.35949071  0.70238812]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -0.06526383]\n",
      " [-0.22140353 -0.53068784]]\n",
      "[-0.009999999776482582, 0.0]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "[[-0.7463788  -0.1147987 ]\n",
      " [ 1.          0.79929558]]\n",
      "[0.0, 0.0]\n",
      "[[-0.3449559  -0.80037982]\n",
      " [ 0.46502097  0.17081731]]\n",
      "[0.0, 0.0]\n",
      "[[-0.54856641 -0.12541265]\n",
      " [-0.38533226 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.42924678  0.00692994]\n",
      " [-1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.36146067]\n",
      " [-0.8293112   0.38353416]]\n",
      "[0.0, 0.0]\n",
      "[[-1.  1.]\n",
      " [-1. -1.]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.50720909  1.        ]\n",
      " [-0.52843233 -0.67874845]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -0.52963418]\n",
      " [ 0.19857643  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.91707961 -1.        ]\n",
      " [ 0.42436964 -0.04221065]]\n",
      "[0.0, 0.0]\n",
      "[[-0.29286811  1.        ]\n",
      " [ 1.          1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.21303808 -1.        ]\n",
      " [-0.31083918 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.45283707 -0.98921906]\n",
      " [ 0.37677914  0.58380756]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.78037944  0.31454504]\n",
      " [ 1.          0.76494356]]\n",
      "[0.0, 0.0]\n",
      "[[-0.13448227 -1.        ]\n",
      " [ 1.         -1.        ]]\n",
      "[-0.009999999776482582, 0.0]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "[[-1.          0.94943606]\n",
      " [ 1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.11291162 -0.57580798]\n",
      " [ 0.51534173  0.07494702]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.54076785  0.37273643]\n",
      " [-1.          0.66761683]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.92511968  1.        ]\n",
      " [-0.06655332  0.04570992]]\n",
      "[0.0, 0.0]\n",
      "[[-0.36243858 -1.        ]\n",
      " [-0.09693196  0.59318024]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -1.        ]\n",
      " [-0.46875621 -0.79623566]]\n",
      "[0.0, 0.0]\n",
      "[[-0.45065498 -0.54613238]\n",
      " [-1.          1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.37551978 -0.16447359]\n",
      " [ 1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.79329155  0.10333178]\n",
      " [-0.28641143  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          0.41726688]\n",
      " [-0.09037099 -0.49765752]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.27018284  0.00926073]\n",
      " [ 0.30316028 -0.7229379 ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.12120599 -0.83797409]\n",
      " [-0.67444098 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.8328367   0.91301669]\n",
      " [-0.09373717  0.03653247]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.17643212 -0.98384907]\n",
      " [ 0.06480786  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.25726284 -0.62999704]\n",
      " [ 1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.79872831  0.26414203]\n",
      " [ 1.          0.11766731]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          0.20914176]\n",
      " [ 1.          1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.20065642  1.        ]\n",
      " [ 0.08628441  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.11695931]\n",
      " [ 1.          0.36131433]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -0.82136724]\n",
      " [ 0.7487826   0.49303874]]\n",
      "[0.0, 0.10000000149011612]\n",
      "[[-0.4744801  -0.4516747 ]\n",
      " [ 0.26581764  0.91222914]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.52786347  0.32410955]\n",
      " [ 0.05221289  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -1.        ]\n",
      " [ 1.          0.38703147]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -1.        ]\n",
      " [ 0.56109271 -0.98992839]]\n",
      "[0.0, 0.0]\n",
      "[[-0.1455856   0.02849952]\n",
      " [ 0.46658651 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.02950198 -0.0602352 ]\n",
      " [-0.26708821 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.2029863  -0.06863717]\n",
      " [ 0.72577798  0.6167968 ]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          1.        ]\n",
      " [-0.10965644  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.78179585  1.        ]\n",
      " [ 0.73145605 -0.13967937]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -1.        ]\n",
      " [-0.45648578 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.42932454 -0.22204173]\n",
      " [ 0.95226624 -0.14152838]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -1.        ]\n",
      " [ 0.94821523 -1.        ]]\n",
      "[0.0, -0.009999999776482582]\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    \n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        \n",
    "        print(actions)\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        \n",
    "        \n",
    "        \n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        \n",
    "        print(rewards)\n",
    "        \n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
