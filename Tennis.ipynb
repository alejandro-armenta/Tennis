{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.36 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mjupyter-console 6.4.3 has requirement jupyter-client>=7.0.0, but you'll have jupyter-client 5.2.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from model import Actor\n",
    "from ddpg_agent import Agent, ReplayBuffer\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like:\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ] \n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.4669857  -1.5         0.          0.\n",
      " -6.83172083  6.          0.          0.        ]\n",
      "Actor Local:  Actor(\n",
      "  (fc1): Linear(in_features=24, out_features=400, bias=True)\n",
      "  (fc2): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n",
      "Actor Target:  Actor(\n",
      "  (fc1): Linear(in_features=24, out_features=400, bias=True)\n",
      "  (fc2): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n",
      "Actor Optimizer:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Actor Local:  Actor(\n",
      "  (fc1): Linear(in_features=24, out_features=400, bias=True)\n",
      "  (fc2): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n",
      "Actor Target:  Actor(\n",
      "  (fc1): Linear(in_features=24, out_features=400, bias=True)\n",
      "  (fc2): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n",
      "Actor Optimizer:  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:\\n', states[0], \"\\n\", states[1])\n",
    "\n",
    "memory = ReplayBuffer(action_size, seed=10)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "\n",
    "# I am making a global actor for all agents!\n",
    "actor_local = Actor(state_size, action_size, seed=10).to(device)\n",
    "actor_target = Actor(state_size, action_size, seed=10).to(device)\n",
    "actor_optimizer = optim.Adam(actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "\n",
    "agent = [Agent(state_size=state_size, action_size=action_size, memory_=memory, actor_local=actor_local, actor_target=actor_target, actor_optimizer=actor_optimizer, random_seed=10),\n",
    "         Agent(state_size=state_size, action_size=action_size, memory_=memory, actor_local=actor_local, actor_target=actor_target, actor_optimizer=actor_optimizer, random_seed=10)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 200\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 300\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 400\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 500\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 600\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 700\tAverage Score: 0.02\tScore: 0.00\n",
      "Episode 800\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 900\tAverage Score: 0.01\tScore: 0.00\n",
      "Episode 1000\tAverage Score: 0.01\tScore: 0.00\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztnXm8XVV96L8/bgYSwpQBxAQMSKyl8sQa48yzWjT4+sD3igXqU2xpU9vytM/aFl4rtlTb59CivlJLFBRQGcQqeRiIMooFQm4QyEBCbgKBSyADGQiZb+7v/bH3uXefffaw1h7OOffm9/18zufsvfYafmvvtfdvrfVbg6gqhmEYhlGUwzotgGEYhjGyMUViGIZhlMIUiWEYhlEKUySGYRhGKUyRGIZhGKUwRWIYhmGUwhSJYRiGUQpTJIZhGEYpTJEYhmEYpRjTaQHawdSpU3XmzJmdFsMwDGNEsXTp0i2qOi3P3yGhSGbOnElvb2+nxTAMwxhRiMh6F3/WtWUYhmGUwhSJYRiGUQpTJIZhGEYpTJEYhmEYpTBFYhiGYZSiVkUiInNFZLWI9InIpQnXPy0iK0XkCRG5W0ReE7l2kYisCX8XRdzfLCLLwji/LiJSZx4MwzCMbGpTJCLSA1wFnA2cBlwoIqfFvP0SmK2q/wm4FfhSGHYy8DngrcAc4HMicmwY5hvAPGBW+JtbVx4MwzCMfOpskcwB+lR1naruB24Czo16UNV7VXV3ePowMCM8/gDwM1XdqqrbgJ8Bc0XkBOAoVX1Igz2Crwc+VGMejBI82LeFdZtf6bQYhmHUTJ2KZDrwXOS8P3RL42Lgjpyw08Pj3DhFZJ6I9IpI7+bNmz1FN6rgd7+1mPf+0/2dFsMwjJqpU5Ek2S400aPI/wBmA1/OCescp6rOV9XZqjp72rTcGf6GYRhGQepUJP3AiZHzGcCGuCcR+U3gr4FzVHVfTth+hru/UuM0DMMw2kedimQJMEtEThaRccAFwIKoBxF5E3A1gRLZFLm0CHi/iBwbGtnfDyxS1ReAnSLytnC01seA22rMg2EYhpFDbYs2quqAiFxCoBR6gGtVdYWIXAH0quoCgq6sScAPwlG8z6rqOaq6VUT+nkAZAVyhqlvD4z8GvgNMILCp3IFhGIbRMWpd/VdVFwILY26XR45/MyPstcC1Ce69wBsqFNMwDMMogc1sNwzDMEphisQwDMMohSkSwzAMoxSmSAzDMIxSmCIxDMMwSmGKxDAMwyiFKRLDMAyjFKZIDMMwjFKYIjEMwzBKYYrEMAzDKIUpEsMwDKMUpkgMwzCMUpgiMQzDMEphisQwDMMohSkSwzAMoxS1KhIRmSsiq0WkT0QuTbh+pog8KiIDInJexP03ROSxyG+viHwovPYdEXk6cu2MOvNgGIZhZFPbxlYi0gNcBZxFsNf6EhFZoKorI96eBT4OfCYaVlXvBc4I45kM9AE/jXj5C1W9tS7ZDcMwDHfq3CFxDtCnqusAROQm4FxgSJGo6jPhtcGMeM4D7lDV3fWJahiGYRSlzq6t6cBzkfP+0M2XC4AbY25fEJEnRORKERlfVEDDMAyjPHUqEklwU68IRE4ATgcWRZwvA14PvAWYDPxVSth5ItIrIr2bN2/2SdYwDMPwoE5F0g+cGDmfAWzwjON3gB+p6oGGg6q+oAH7gG8TdKG1oKrzVXW2qs6eNm2aZ7KGYRiGK3UqkiXALBE5WUTGEXRRLfCM40Ji3VphKwUREeBDwPIKZDUMwzAKUpsiUdUB4BKCbqkngVtUdYWIXCEi5wCIyFtEpB/4MHC1iKxohBeRmQQtmvtjUX9PRJYBy4CpwOfryoNhGIaRT52jtlDVhcDCmNvlkeMlBF1eSWGfIcE4r6rvrVZKwzAMoww2s90wDMMohSkSwzAMoxSmSAzDMIxSmCIxDMMwSmGKxDAMwyiFKRLDMAyjFKZIDMMwjFKYIjEMwzBKYYrEMAzDKIUpEsMwDKMUpkgMwzCMUpgiMQzDMEphisQwDMMohSkSwzAMoxSmSAzDMIxSmCIxDMMwSlGrIhGRuSKyWkT6ROTShOtnisijIjIgIufFrh0UkcfC34KI+8kislhE1ojIzeE2voZhGEaHqE2RiEgPcBVwNnAacKGInBbz9izwceD7CVHsUdUzwt85EfcvAleq6ixgG3Bx5cIbhmEYztTZIpkD9KnqOlXdD9wEnBv1oKrPqOoTwKBLhCIiwHuBW0On64APVSeyYRiG4UudimQ68FzkvJ+EPdgzOFxEekXkYRFpKIspwHZVHSgYp2EYhlExY2qMWxLc1CP8Saq6QUROAe4RkWXAy65xisg8YB7ASSed5JGsYRiG4UOdLZJ+4MTI+Qxgg2tgVd0Q/q8D7gPeBGwBjhGRhgJMjVNV56vqbFWdPW3aNH/pDcMwDCfqVCRLgFnhKKtxwAXAgpwwAIjIsSIyPjyeCrwTWKmqCtwLNEZ4XQTcVrnkhmEYhjO1KZLQjnEJsAh4ErhFVVeIyBUicg6AiLxFRPqBDwNXi8iKMPivAr0i8jiB4vg/qroyvPZXwKdFpI/AZnJNXXkwDMMw8qnTRoKqLgQWxtwujxwvIeieiod7EDg9Jc51BCPCDMMwjC7AZrYbhmEYpTBFYhiGYZTCFIlhGIZRClMkhmEYRilMkRiGYRilMEViGIZhlMIUiWEYhlEKUySGYRhGKUyRGIZhGKUwRWIYhmGUwhSJYRiGUQpTJIZhGEYpTJEYhmEYpTBFYhiGYZTCFIlhGIZRClMkhmEYRilqVSQiMldEVotIn4hcmnD9TBF5VEQGROS8iPsZIvKQiKwQkSdE5PzIte+IyNMi8lj4O6POPBiGYRjZ1LZDooj0AFcBZwH9wBIRWRDZMhfgWeDjwGdiwXcDH1PVNSLyamCpiCxS1e3h9b9Q1Vvrkt0wDMNwp86tducAfeHWuIjITcC5wJAiUdVnwmuD0YCq+lTkeIOIbAKmAdsxDMMwuoo6u7amA89FzvtDNy9EZA4wDlgbcf5C2OV1pYiMTwk3T0R6RaR38+bNvskahmEYjtSpSCTBTb0iEDkBuAH4PVVttFouA14PvAWYDPxVUlhVna+qs1V19rRp03ySNQzDMDyoU5H0AydGzmcAG1wDi8hRwE+Av1HVhxvuqvqCBuwDvk3QhWZ0GapedQbDMEYwdSqSJcAsETlZRMYBFwALXAKG/n8EXK+qP4hdOyH8F+BDwPJKpTYMwzC8qE2RqOoAcAmwCHgSuEVVV4jIFSJyDoCIvEVE+oEPA1eLyIow+O8AZwIfTxjm+z0RWQYsA6YCn68rD0ZxrEFiGIcOdY7aQlUXAgtjbpdHjpcQdHnFw30X+G5KnO+tWEzDMAyjBDaz3TAMwyiFKRKjFqxnyzAOHUyRGIZhGKUwRWLUgg3/NYxDB1MkhmEYRilMkRiGYRilMEVi1IJ1bBnGoYOzIhGRd4nI74XH00Tk5PrEMgzDMEYKTopERD5HsDjiZaHTWFImDBoG2Mx2wziUcG2R/DfgHGAXBHuEAEfWJZRhGIYxcnBVJPs1GM+pACJyRH0iGYZhGCMJV0Vyi4hcDRwjIn8I3AV8sz6xjJGOmrndMA4ZnBZtVNWviMhZwMvArwCXq+rPapXMMAzDGBHkKhIR6QEWqepvAqY8DMMwjCZyu7ZU9SCwW0SOboM8xijBRm0ZxqGDq41kL7BMRK4Rka83fnmBRGSuiKwWkT4RuTTh+pki8qiIDIjIebFrF4nImvB3UcT9zSKyLIzz6+FOiYZhGEaHcN3Y6ifhz5mwS+wq4CyC/duXiMgCVV0Z8fYs8HHgM7Gwk4HPAbMJRootDcNuA74BzAMeJtg0ay5wh49shmEYRnW4GtuvC/dRf13otFpVD+QEmwP0qeo6ABG5CTgXGFIkqvpMeG0wFvYDwM9UdWt4/WfAXBG5DzhKVR8K3a8n2LfdFIlhGEaHcJ3Z/h5gDUEL41+Bp0TkzJxg04HnIuf9oZsLaWGnh8dF4qyUF3bs4Ut3ruIfFz7J9t37AXjyhZf51gPrEv0vf34Hf/GDx9m6a39qnA+s2cxtjz3f5Hbg4CB/8+NlXPrDJ9g3cDBTppUbXubaXzw9dL57/wBf+MlK9h5IDvfg2i38oPc5vrxoFZt27k2Nd+n6bXxv8fomt3tWbWThshdSw9z+xPC1vQcOsmLDjhbZ/mHhk6my+dC/bTd/fsvjfGXRah5a+1LL9TuXv8hdKze2uN+7ahN/8+NlDByM12OaWfLMVm5e8myT2+CgNt231S/u5DM/eJxNL6ffxyg3PvIsS9dvTb2uqnz1rqe45PuP8tIr+/i3+9fytbvWcN/qTUN+9h44yD8sfJJd+wac0kzj1qX9PLzuJQ4OKpfftnzoXq3ZGORp48t7uWfVRr521xrm/3xtU9hHnt7KLb3PJUU7lI8vL1rFJ2/8JTt259U9h9+h+5/azILHN7Csfwdv/vuf8dFrFnNwsJjhbc3Gncz/+VpUla/fvYZnX9qd6O+nK15k0YoXAbj6/rWs2bizxc/3Fq/n0We3NbkNDipfWbS66dkvf34H1z34TJM/VeVrd61hzcadfOEnK9m1b4B/+ulqXtzRXGZueHg9jz+3fej82l88zcoNL6fmb3BQ+dKdq9i8c1+qn7px7dr6J+D9qroaQEReB9wIvDkjTJLtwrUkpIV1jlNE5hF0gXHSSSc5JuvOp258jEeeCT4EL+3az1c+/EbO/toDAPzBu09p8f/1u9fw05UbOfv0V/He1x+fGOdHr3kEgHPPGNaNtz22ge8+HHzETp9xNB9562tSZfrg14P0f/9dwTJo37hvLd984GmOO/Jw/vDMVpl+95uLh46XPf8y1//+nMR4f/sbDwI0pf373+kF4Jn/818Sw3zmB48PHX9/8bNccfvKFtnm/3wd0yaNT5TNhzuXv8gPHw3qF/9yb1+LTJ/47tJEWT950y/ZuXeAP3z3KbxmSvoc2w//20MAnP+W4XK0+OmtXHXvWlZseJnv/N4cvvXAOm5d2s+7Tp3Kh96UX7e57N+XJcrU4Pnte/jqXWsA2L3/IPesGlYgjTDfW/ws83++jjGHCX859/W5aabReFa/+Kvf4PqH1nPzkudY/fmz+faDz3Dr0n7mnDyZv7z1iSH/88587dDx71wd3JvfmX1iYtxPvrCTq+4NlM/hYw/jS+e9MVOWxjsU54E1W7hn1SbOOi353cni3Kv+g937D/LB00/gn3/2FD9+7Hnu+fP3tPibd0NQTtb+wwf5xztW8fW717DiirlNfv76R8uB5ue29Nlt/Mu9fTzev50bLn4rAL/1f38BwEXvmDnkb8OOvVx511NceddTAKx/aTc/XbmRxU9v5ZY/evuQv8/+uDmNxruTVlYeXvcS/3rfWla9uJNrP/4Wt5tSMa7G9rENJQKgqk8RrLeVRT8QLV0zgA2O6aWF7Q+Pc+NU1fmqOltVZ0+bNs0xWXf2RloHB3JqtAB7wpq372img4PDcR/mOa5gfyjXgENNbn9Oa6cMSTVJH9mKxO/Czr1BTb7ICLPBMND+gSAf+x3KgA9RmdJaogMV3sNomvsaeRpIzpPPpmWDEb8HDpaTs+hz3r2/+d3bd8DtWe1xbC035NqXcr8ajD2s+f0djN3vohyMlcVO4KpIesMRW+8Jf98EluaEWQLMEpGTQ/vKBcACx/QWAe8XkWNF5Fjg/QRzWV4AdorI28LRWh8DbnOMsysoMyx2bI+t+l8HNlLZj5E6tNtV7rp29+yJKZJxY0bPgFPXL9MfAyuATwKfIjCYfyIrgKoOAJcQKIUngVtUdYWIXCEi5wCIyFtEpB/4MHC1iKwIw24F/p5AGS0BrmgY3kNZvgX0AWsZYYb2MkV03JjOKpI8W8JIxbYFDnD+0NYrRm24LtvTrvyNG0UVQ1cbyRjga6r6zzA0tHd8XiBVXUgwRDfqdnnkeAnNXVVRf9cC1ya49wJvcJR7VDG+w4pk/8FBxoyiwm8YnWT8mJ5Oi1AZrl+Fu4EJkfMJBAs3Gp6Uqf12ukXSyT7YOhmpNeyqca6xj9AWnHvXVk3px847/T5XiWtODlfVVxon4fHEekQa3ZQpo+M73BooaxTsVrr9u9gu+UZ/15arv3pyGL+/o8nm6ZqTXSLy640TEZkN7KlHpJFPVo2tzEchbqxrF41kXUe7jDy6+9PYNkXi6q+7b1cqri2p+lokzRGPphaJq43kz4AfiMgGgvL2auD82qQa4ahC+kjd4qW0U+/vuDGHsffAYO6EyJFKlR+OOmqz7drbxflD2+WKN42OSx0TYFzPITJqKxxV9arQKP564GZgALgTeDorrDHM/oFBHlizJfX6Vff2ccH8h5zj27B9D394fS+795eb0ZzFc1t3M+/6Xjbv3MfesCXSmEjVvy15ZnCDGx95NvN6lP97z5pcP3cuf5Ev3rnKOc66uOT7j7LqxfQZxkV4cG16uaiSrbv2c/F3lrB9936uurePf72vjz+4bgkPrNmc6P8f73hy6Pjvb1+Z6CfKJd9/tGVUn6py6b8/kRIiGAX4Rzf0tqya4MPlty3nomsfaXoXvv0fT3PDQ89khtuwfQ9/cF3yO3TH8vQVG/JYu/kV/uR7eTMjAm5ckr4iACTf0ygvvbKPi7+zhB17klcMuO2x5/lq+M7WTV7b6mqgsabH24H/TbBMyjZgfo1yjWjiNZ/okgpJlb4vL1rNw+vSl8uIh/3inav42cqN/HRF67IfVfGFnzzJT1du5M9u/uWQ28JlwfIR/3hH9ke9MWvbhcZksSw+8d2lfOO+tbn+iuJaU739iRf4s5sey47Ls9obXV2gqjiTuOYX67h71Sa++/B6vrxoNV+6czV3PblpaDUFaL4PV98/vNRPY+Jmljy3P/ECT218pcltYFBZ/ny64t22+wCLVmwcmi1ehOsfWs/9T22mf9twT/vf/b+VfPa2FS1+o3J/edFq7npyI3cuf7HF3/+6+fEWN1cu/eETQ+9JS/qx87wlTW5/4gVWJyzT0mD+A8EzTau4feqmx4ZWR6ibvK6tnsj8jfOB+ar6Q+CHIpL9Rh3CBF0Eyc3WMt8En1E13bq6fjfKVmnXVg39J+3qkmm37aPKLjI32TvbuVXk/o4Ue1Rei6RHRBrK5n3APZFrrvaVQ46sZ1+qYHR4+GIVdKNslX7QKoup7kiLJ1TZM2yzbarTZW+k2pZcyFMGNwL3i8gWglFaDwCIyKnAjpplM2gu/O7DF+PnXfiSddE7VUVNsc77l/f8unVeR55Y/lKXGw3Z8FL1/er0/JSh+Dv4UmUqElX9gojcDZwA/FSHn8BhwP+sW7iRSlaBGc21EhdGe+679aPugvs8kmpGd3V7l2I3MFLylds9paoPJ7i1ZyjACCXrBfItGFFzgt+ic8MBJcVek5ZOnQzJ1kVmkiIva/x+Nc7reO/z5KvK5lT1PJL8Fonv3UrPp0+ru2obnWt0RcqGzz1yec/rYvTMiOkislskJeLtskXnitCNslXaSjwEjO2VmUja3CLpdG9AkdbqSGmRmCJpM2W6PrqlL7YM3Shb109IbNNNq3qtreptJOXoRNmL3qtCtrgKZakTUyRdTjFju2ae56XTFkbKG5JCe43tOde7UTvXQrl8Nm5Tp4ztddPJFpcpkhroloLVjXS6e6FuRvKzr7prq52jzbr1vpeVa6RUEkyR1EB9xvZiXQrdZWxvJNie9FwwY7tbOlX787/vJY3toZ92GtujUtXdtTVqje0iMldEVotIn4hcmnB9vIjcHF5fLCIzQ/ePiMhjkd+giJwRXrsvjLNx7bg681CEuob/joy6ycij/TOs/eJpm7HdecZrpd4qwcnY3mkbSZGZMyPkpa9NkYS7KF4FnA2cBlwoIqfFvF0MbFPVU4ErgS8CqOr3VPUMVT0D+CjwjKpGl2T5SOO6qm6qKw9dxygwtncjdr/qIa8FXe0gh3wGO/ygR3M5q7NFMgfoU9V1qrofuAk4N+bnXOC68PhW4H3S2u68kGCG/Yghq7z4FqbmmmmxiWDdZGwfSqeLXqpC4/tTjO1VtW6a4sn9IFeUpnNLo5ph6P73KqvL2L2Mt9PYrinH6XHFfbnLOlqN7dOB6DrJ/aFboh9VHSBYdmVKzM/5tCqSb4fdWp9NUDwdp66NrUYDZmz3j6fb7ljnbCQZcTn46USLpEzFsUj4TlGnIkn6wMdvS6YfEXkrsFtVo+tMf0RVTwfeHf4+mpi4yDwR6RWR3s2bk/dcqIvMFolnXMVmtsfiGOXG9rLvWpEaat3Gdh8jbduN7c4RVhTPEBnGdhcbSSOWthrbIzYSl1ZTznmmHKPU2N4PnBg5nwFsSPMTrjJ8NBDdmOMCYq0RVX0+/N8JfJ+gC60FVZ2vqrNVdfa0adNKZMOfrFpFlRMS02r33VyJ6cSoptzw1YgRRlZVN1M5I22hNKuekNjG4b8uT7EdQ2kz332n8LFu6S4dRBCnTkWyBJglIieLyDgCpbAg5mcBcFF4fB5wT2NhSBE5DPgwgW2F0G2MiEwNj8cCvwUU3xVnhOFaXkbK2PNuwW7XyMS3nA92+DkXGv7rEKgbOvdr21NEVQdE5BJgEdADXKuqK0TkCqBXVRcA1wA3iEgfQUvkgkgUZwL9qrou4jYeWBQqkR7gLuCbdeWhMPGuJUm9VIpONmWLUoeS6ya7Sye6tqqi8gmJlXdtpcffLTX3sh/17inJftS6OZWqLgQWxtwujxzvJWh1JIW9D3hbzG0X8ObKBa2YzA9biZLi3qXQvXRj11aVUtVibG+XInH1V5HCqbJLstiIKM/0Si+66D96spvf5Sg2s70GOj0hsZu7arpRtkpHD1UWmSYc1UtVtg/3+Kr7sLuIXrZrq9jMdD8ZW4bud+H7koQpkhrIHLVVqkVSgQCdpgbZStc0K5KjyriqGqDhlWbFHtvaInGaR9L+cuJvbI+n2c0v8zCmSLoc36Zx4Eszz/PTaQNd9H4UM4KWj6MqunVwRfU2kvTauktcjRZJ0fuVFq7qNcqKMlonJB6yZE5IbKMcVVC1Qb+e/Toqj7IwldlIqonGL82qje25w38dI3JJq0C3UTvoIvNdrZgiqYH4sy9jOM2akJg6jyQ+aswmJGaHb/OERJ/lPOLHybJUdTNdbR/VRFd2q90m+4NHq7vo/UpLIfMd9ZwPVMTYPpSvUToh8ZCl48b2wikkxVVtlagbR21Ver88lUSqH8+PZBW0fa2tKo3fTsb2ssb9AmE8w8fvbacXmnTFFMkIwr0vdmQUvm6h3bfLnk5nKF/haL8icqEbJiSaIqmBeIErsl5Wkv+WeFOasi1da11kbB9SclX2j5d9wYuYfUsY26vu2qps9V9XfykefYeu+otdztjekK+4sT3fPf5Rz3p/fdJwwYzto43Mrq1Dm27s2qqSWiYkVhOlV5pJ50PuruHzjO0VVgCc5pEMlkquGL4Vx9h5p5d1ccUUSQ1kPnvPL42t/usQZwciSDe2O9Q6nUSKfiSzQ1S3+m+sxp86mKMao7y/0o0Z231r+41YOrb6b35c8XvrYyMxY/soI9vYXiJeZ3/dW42pRbZumpDo9LHw89O2FkmuQ+js6F633L6G7LqM7Znve8kWyUixd5oiGUE4F6oKy143LQxZ10vVfmP7yPg4lKXbttotP7M9pYVW9bDpCB3pjiuAKZIayDI6ljG256Uz7O7mzzWdStHYv09Qzz5753grNLa7fdB8E8uLryJje8lovI3t3vc9Hr97919UnqqN7WS8302tpprmkfjEXxemSGogu6k7smqj3TSPxNXI20lGtLHdcQ5DkS6eKvy3hE85TqMThmvv9z3m3aU7rhvKvymSGsh6rr7P3IztDnF2YHx/2hDvqhRvJ4ztcdHTK+BuLeHqSTe2u9CQ2/V+tbbs3fylXSsyIdFHEZmxfZQRf/hl5pE0xRv/eKXOI+mCKkoKpWb2e9aQneMtJIufexk/nTK2++bRd5vY0vnybJL4tkhauy/zy1/2PJIiafqH6QS1KhIRmSsiq0WkT0QuTbg+XkRuDq8vFpGZoftMEdkjIo+Fv3+LhHmziCwLw3xdqh7LVwN1dHe0I71uo65sFdqwqEx6Xazoy+Brm6vS+O1mfyibXr4cVad/yM8jEZEe4CrgbOA04EIROS3m7WJgm6qeClwJfDFyba2qnhH+PhFx/wYwD5gV/ubWlYeiZE3M8jYvZhjyUsO0nHdPP+tQOl1lbC8QxrP27evXp0ukLmO7b1eOb226rPr2fTfqMra7DohxapFkhM9LY7Qa2+cAfaq6TlX3AzcB58b8nAtcFx7fCrwvq4UhIicAR6nqQxqUhuuBD1UvesU0FfgRUsWoiTpq8t10S6v7qPvVtitJ09nY7qZi8qSu1Nju1LXVgYLi3Z0WfwYuYfzSqIM6Fcl04LnIeX/oluhHVQeAHcCU8NrJIvJLEblfRN4d8d+fEycAIjJPRHpFpHfz5s3lcuJJlROzmuwrzum72VLS0vEN68OQbAWiTW+RtN9IEg8yZGz3bG24+MmLs7qZ7Y7+nOPLVSWOMTVIz6eP/aHw/SrQIvZVdq12KpcWScBoNbYn5arl/Uvx8wJwkqq+Cfg08H0ROcoxzsBRdb6qzlbV2dOmTfMQu3qqs5HEa3zdX0MfCRRSRCXu8WhtlfpWoEq3SDznkZSe2Z76vkXlyI7BFyeZu6A41alI+oETI+czgA1pfkRkDHA0sFVV96nqSwCquhRYC7wu9D8jJ86Ok7XVbZmx9kXLSxkbSeXzSGqwkZRukBTSI8n99UX6wRP9ePT/17b6r6dNoLU27ZleLuldZz73vXIbiWOYIiOwfIYMj1YbyRJgloicLCLjgAuABTE/C4CLwuPzgHtUVUVkWmisR0ROITCqr1PVF4CdIvK20JbyMeC2GvNQiKzCUOphu3Y9dEENpQ7qmr9QSJEUMLz6pedX264C9wUDXVvC2XKXncDo/5Eu2yLxvRCrRDqlMTJtJGPqilhVB0TkEmAR0ANcq6orROQKoFdVFwDXADeISB+wlUDZAJwJXCEiA8BB4BOqujW89sfAd4AJwB3hr6twraEcitSR/27qKqpsQmIX2IX9Z7a3V2jfe92Uj6NKAAAcx0lEQVSZme2+AZpPR8oOibUpEgBVXQgsjLldHjneC3w4IdwPgR+mxNkLvKFaSaul1ZaRfOxCs7G9ObTrhMSumtnekK1KY3vZrq0iYWKBfIztLgn6lJm6ZranfXhd2ylVd8llN3jcu28L79meplgjabdMSExIPzONXIf0MKnfA9XKl86PYzPb20C7F9UbIZUYb+rKVhUTEr3mkXSDddQB7w99vOspz7+fOJnxt2P4b6oCdX4v/dOvokXSju+BKZIaaPnIRI9L9Au7Bm1N3722VjdJxnb3jZJqspEUCZPS6nRqkHj2e7fP2B7Lk2cLsO6tdjNX1XYK3whX7H4VmpDYNB/IPw23MBr6ref9cMEUSQ20FKymAl/8sdZZ82kXZSQrWyNMjbdA+NYWiTpH5vNxgPb1k8eTSbWROD7Fqrfazf5gu4Qv2yLx/1B7DwhoMbaXL0/t+B6YIqmFeG01UiupsMCPlC6SKEl5K9tl1033oSpJOpEj19qw62Cu3OdatgLQNCIqP7LBNljbq67UVDFqy1okI5QiY8HT/GcZ21PDx87LGNur3qFtSLYCM/bTPJavcBWwkaR8dKuqKPh021Q2sz12nmpsT1Xobv7S/OeRFb+PIbvyme0ZifsO/U9t6WaFCf2kG9tzoyiNKZI2UNWD7ISxvR21/U63wIrdrwKd2f5eO0p5Y3tO15Z3BSve0k8+TqNsg8SxIeYfQQbtaEVVgSmSGsgwkbjVMJr8p8frKkGpme0Vl+NEY7tv2IopEm3aR7OqgQ0+3aF1LBSZla7r3uX5LZKSNouoIduj5t7Vq/9mKMv0NFrTSpOhLkyR1EBtRkHn0U1O3tziqi6q1BjL9yuX/SAVCJMSRxGDal4CVRpLsz96zdS91a4vVXYZF0q/QJvEu/st5qeKBol1bY1QsrbL9Klh5PprR6u35eWt/qNd1PaT594JqpKlrjxlxes6aqtI3C7p1U1SfqqWIWtCYhF87WmdwhRJGyjznLP6gV2NnmWM7b7dFc4U2ou+JhtJFcZ2nygq/3i5G4995PTfmta3a8sz/oyyWNTY7vXYSrbEqihnWZixfQQiKceQ/ZFpvZZdS8oMmyJbw18n9yeIUmRPlSTy8ls43kJdW0GgoaVRGjYSv14rJ5nS/Fe/6oVbASv64Wx9TzxbPI62iDhey9dkpZ/jnrjHRZMdJzl8NJzvgAUXP2Yj6WL8WgrRrq38Wlva+PjhYX7N52nhyxiA01so/stkJSrDIh/v1BZYye42D7+N2mzcJjL0X4uxPdtW4fJBdqlUuH7EXCfm5VV8ynaFuQ5IaFxrdG2pau47lBxPznNIupZynO6n2VdWq7DxTPPKnrVIRig+TfDEwpfWIkn5zwpflDTjsWq5z3ZS6LJdA+001g8vRxFzbznIiMNT2VRicHWoVMSv+M4jcWzQeHjI9u5vyI5W0PxFcKnIZPdG+FcysuxUrq3gNugRUyR1kKUsWl+GEo+5DSUkXpDLJlnG2J7a1VJcnMqpSpa68pSllFw/Yul6JF6BqrfLxaW2HyUx77UXHr8E8ns3WumGpeZNkbQBnyGXWdedux7i3VSljO3ZcRemiLG9oqSrwNewXNSvC14ztT0S9ze2x86rEyX0n66ofGr7zcb28g+jzvLrMyExaxn5ujFF0m58m7eeQ4cDf/FWhH+TOs29/CSyoYgicZajE8b2hFjCuBzutUtsHh9Jr665zGsZ/TIO6dX9vUrtTnQO3/qMqqgAZN5Tz2ZTkQmJw379WpBVUqsiEZG5IrJaRPpE5NKE6+NF5Obw+mIRmRm6nyUiS0VkWfj/3kiY+8I4Hwt/x9WZhyJkGgVz/Gb5d555XGHJKVPzTowv0UbiFmm6t7JdJP7h0/rCffvq02VKT6sMLkbpBvVvbJV9PS8Bl5FtUZLyU8mHOiJI5sZWLhW6lrgdhcuKsw2apLYdEsM9168CzgL6gSUiskBVV0a8XQxsU9VTReQC4IvA+cAW4L+q6gYReQPBdr3TI+E+Eu6U2JX4GduTPqyu6dRPXsvad/e1ZMXpGLaDo1JcqUyUmvLkZ2z3u9+tCjKnJZV51QU/TVJm5emi+HffNZ87LSOf24foJ0MR6myRzAH6VHWdqu4HbgLOjfk5F7guPL4VeJ+IiKr+UlU3hO4rgMNFZHyNslZKtrE9v4bvWht1LaRVTkgsIkNiHN201W6B8K3dh+5x+d6zPO+Vrf4bEyy9ElG2BZmcXm58mUrQ/YNbfKvdAmEyRnQlX4h/H1xaMYEf162366BORTIdeC5y3k9zq6LJj6oOADuAKTE/vw38UlX3Rdy+HXZrfVZSSoWIzBORXhHp3bx5c5l8lKaMsSupP3doDHyNNfS0SVyqhXRAJp1ugfncr/g8kuE4uqhZlECRyall91SvuqKcPbQ2P3y0dp/3DiWm7yhXavgCRcRlHkkd6fpSpyJJnOjp40dEfo2gu+uPItc/oqqnA+8Ofx9NSlxV56vqbFWdPW3aNC/By5L13LJehmG35Nro8FyA9LBRf2nnPnLFC7LGYnMpo4kvvG8kpH/Y2jkhcXgF2eQ4KpuQ2HTP0vKdfb3Zb7LcRXAemJHbIvFMN+PcqRwOpau571BieIfyl6nscuRKDO/QChuyz41SY3s/cGLkfAawIc2PiIwBjga2huczgB8BH1PVtY0Aqvp8+L8T+D5BF1pXkWWgbn0ZEvpt08JmdJmlpZd07kOesd3bcJyY33JdJeW7tvwjaHmO2vyfHdZP2XhPDMygznkk/t0yni2eClskRSQo0iJxqhBkfB9cRv/m2VFG+vDfJcAsETlZRMYBFwALYn4WABeFx+cB96iqisgxwE+Ay1T1PxqeRWSMiEwNj8cCvwUsrzEPhchSFm4tEteE6i8geRMSC7dIUq4XIbsF2I76WPW41bCLKMD8Gm4D7x0SHSs6efG44rvVbmJ6NZcP37WyChnbvaWqntoUSWjzuIRgxNWTwC2qukJErhCRc0Jv1wBTRKQP+DTQGCJ8CXAq8NnYMN/xwCIReQJ4DHge+GZdeaiKzBpLXtgMvy5NZVdSt9ot0AJxSzASp2OQut75ItGWGu9fcT78JiS6ey27jHxu15ZX7K0fZd8WydBadR1b/dcfp7hDP+nG9vqpbfgvgKouBBbG3C6PHO8FPpwQ7vPA51OifXOVMhbFZ/XfpmsujzWlOexu1AtfmKFF3YoXpdwWiWdXTnKlsFxRLz2yrUDyjSDDgxK06b9scs12svwuEVeyW29unotOSCw7SMPXvjiUrgR+y7eGs7v6klf/zU+ruWvLt3uw/pafCzazvSBZLYXWvuJ0z4lj21MLbEa8CUl4LW3u2l3R0n3h16UwlN+ml8eNIluJ1tVF1Aji250ThPH7OOQbrd3lz/Jb1kaSN8ijSCUkM3xCBSVLWQwOPbNo5czn3qW565AM2Y3DfAVcxL45nL7/+1EVpkhqIOvj4lLpSytYrobuIh+3NPKX5/CMr5Qs/pHWtcNcmlJ3a6E5+HGqyfoL7tPNWtrIn6cAfUtDyaZn2cUNCxQ/p+c42KTY4tfy5cr1Yy2SkUlrzSu9BpRYg8qJf3gMfH0Mb9gUQ4vsR5L+oqS5Jcbj6Z53rQh59ojqan/VKsChbs7M+LJbFLnunhWY0sb2hFata/7i71C37L7ZOuAhPXzjmeaO2vKSoBimSNpA5oMsUGvLasYnReryjrSsETTUHdD6camkYDYZ291irO9l9/A7ZAuJx1G+i6QoLsb2hnw+tfLBQT85fLuufG+DS80/s+su/BcRh3coPY0swcoMrElOMys/zWXRttodgXgZ27O6p5IURUoN3nWkULxglfn0xz8mxfpwm84SPLjJkp7fjJfNWz4/WeIrALjF5dfaqHKiWSUfOseutrq32k16N5JizNpq10eh5LXQihrb0/xH5csOk9ciqV+TmCIpiKYcQ/YL5VJrS4vb2UYSS7ecsT1befmuaKra6uhazPMGFyRfiyplv64aF1la/j3CZvpx8O9SC4+T2SKJXfKfkBjzV+CjmB1f/nuVbWxvvA+tZaLMc8t69s3vcsr9zEjcpQU5NIigLZ1YyZgiqYEsZeHycqUa213Tr/AFzptH4quk8vKbE1Nu/Jlp53wIfEgboVSHsT39g+4ueFpXZVZ8/qv/xuNzk8kVl/cqq+KQPavfr5WYJVdavPUZ23NaJG3QL6ZIaiDrhXIZJ152yZDEhAtSycx2sl+m0kukOIVuF9VIU9WaXcPxBfgtkZIWl5ti68RWu5mVtqR3zaMlWQS39yN67HcPwWGrBwcZymKKpMtxWoY6HqaAIcN1q92q6MTHPy3NIrIUsRUV8Vs1I0XOJMrKM2Q7LLqMfIEWcXP4Amk6BQo82Va7owy/ja0SI0gOGzPq5Rlh4/9ZxOVyXUbet0sgaQZ42RfR1die6s/jRRtaRj5FtjJG2zSR8hSgX/diVtdWvktWeu3v2krvxkr0z3DZi79DpeyIkZTSRj+myRh396qgaLOfTm78ZoqkDlpeKLcPXUrwVL+uRtgyBSlvJI5/070Rr18c8TCuMrhsEFXo9qR8Nct8kBKiy/bvYygOfWUN6XXv2nILnyeYtyLJ6WaNx+liI8n7CLvJlXnVK3zcd7ZNK99PuzBFUgOtNafocfaHOcu/a0vDdZ+CLFKNs+r4kUuKrEmm6GU3+VLzkRG8amN72iif4b1i/D4ceelk+vGIj7Tn2eQl/uHNr0EnyZMWX55/X3KN7TH/g1nvUqkWSbofl/LX7D9f+cX95m+HnZ9uWUyRtIGSFZb0oG0oIK0bW7l/+F1xt/1UmuwhxXDt1T3MoI9nEipJuR+40qokctTa+nZpIXV7mXLpts2f2V5/Jk2RFMRvQmJ6LSmxeZ5mYHdozUQj9altpS8j3/pxaJYplnROX3WZnfpc+qhd0m6Nt3iLrciExKpebJ8Wp8vqxK42Dlf3Frtbhv8iw2+TWyTR+IOz+HI/zcvIuz+L9PctTCcxTH74jNcpp9u2+cBmto9Ash9+/IMfOc54GZLcMtPJaWonvVxpuBtQs5VZXn6GlVyKssySMVURZITJMGTGRPIibn+Kd5Vkh63Ij4dSdqm9upYvV02S+1HMaj0kRp/xXg3F0/q843nP8pOZft77lhMm/R1LL6OZzyuerza0PNKoVZGIyFwRWS0ifSJyacL18SJyc3h9sYjMjFy7LHRfLSIfcI2zG8isOeV8iAO3lLCuH9zYx7pMjSSvReJSi82bgVznVrtVfdjz4q1iYENz/P419Oz4AjJ7q1r659Nq0G7uPvNIijynvBZJnEx7g8v9dpSr+Vp+RaZZrvgzyJJHc/20xlgPtSkSEekBrgLOBk4DLhSR02LeLga2qeqpwJXAF8OwpxFszftrwFzgX0WkxzHOjuPxrqb4ib5gycfZ6cdry8WLUm4/t2ccdTSzXUfFVdsiSY6jOmN71rVGDbT53IXMshk7953ZXjdlk3Wp3Wemn3o/3Fp56Q25Yi0SVz8jfR7JHKBPVdep6n7gJuDcmJ9zgevC41uB90nQgXkucJOq7lPVp4G+MD6XOLuOsosKxhkaA19jARnuV27tTsj+cPvL1O0GzyhFJ7N1muEuHA+l4/lcXG0sSf6LlZvWSlZmxSFyXtc7VHnxyKxQ5Hpxul4FUtfHSETOA+aq6h+E5x8F3qqql0T8LA/99Ifna4G3An8LPKyq3w3drwHuCINlxpnE7Nmztbe31zsPf/2jZTzy9NbEa2s2vTJ0PGFsDzOOnTDk9uqjD+eI8cO7GG/bfYAtr+wD4KjDx3D8UYcPXRsYVJ7esqsp7plTJjK2J9DxL+3az9Zd+wE4duJYpk4az/Pb97B7/0GmHDGOyUeMa5Fp+jETmDiuh00797FjzwGOmTiWaZPGZ+YBYNZxk4aO17+0m/0HWycdzJwykee27eFg2J4+ZeoR9BzWvAd2XyzeYyeOZdvuA4myAbxmykTG9aTXaRpynjh5AoeP6Wm5vmH7HnbtP5iYj4OqrNsc3N/XTjuCwyJveiPeqZPGc+zEsbnpR+OPPtNZx03i2a272TcwyJHjx/Cqow9PimaIvQMHeW7rnkR5G7y89wAbX96XGP7U4yYhwJZX9rFt9wGOnjCW445sfb5RGvd70vgxvLJvINHPtCPHs3lncppRouU7em+OO3I8R08Yvo+v7BvghR17h84njuth+jEThs537DnApjC9Rp7i9zrK1EnjOHbicHnfvf8gz28P7mPjXUh63o04o3k/YlwPuyLvUDRclOiz2XPgIP3b9rS4R8tCnEZ5h+xn2ogvmkZc5ng5ib5DSfI28n3Xp/8zpyaUMRdEZKmqzs7zV+ee7XmDGLL8pLknfW0SNaGIzAPmAZx00knpUmbw6mMmMOv45Afw2mmTeHDtFhR496ypAMw4dgIrNrzMGScd0+J/6fpt/OoJRw0VqihvnHE0k48Yz2PPbWPn3oGmNGcBj67fzq+86kiOGB+EnXX8JB5Ys4W3njK5Rd5VL77MG088usnfO147JTEPx0wcy0u79rN55z7ederUptrUrOMn8cBTW3j366by6PrtnHDM4ew9MMjJUydy2quPYuGyFzl9+tGcOHlCS7xHjOvhyRd2MrZHOHBQeftrp7Br38EW2e5c/iKvO/5ITpl2RKJ8DRrK8w3Tj0q8Puv4SezYc4AXtu9lyqRxTIt9VA8f00PPYdIi6+tedSRbX9nPsUekKxGAyUeM48WX97Jt137e8dqpHBaWwgfWbBm6bw0Zoh/SLAThxR17eeepU5iQUCYAHlz7EgcGBjnzddN45OmtHD1hLEdOGMv0Yw4fyvcDa7bwzlOTn2+Uht93z5rKmo2vcOThY1iz6RXe8dopLFqxkXeeOoWjJ4xl6679jDnsMNZv3cXp049mx54DPL15F4ePbXwMB5rK97FHjGNjeG9mzzy2Jd0DT2/llX0DjDnsMM583dSW69t2HWDypHFDLYPGO/TqYyawe/8AM46dyD2rNnHMxLHMOXlyS/h3vHYKh4/t4aVdwx/o+PM+afJEHu/fzpyTJ/Pg2peYM3MyY3qk5R2aEOZx8hHjeGDNFs467XjG9rR+il4zZWLLc26UBYBf9G3hjBOPYe2mV4bKezy/23btZ//AIL3rt3H2G17V9O6pwsypE1nWv4N3zZrK0vXbeP2rjhp6/xtEn2m0LDbYuXeAnsOESePr/MwH1JlCP3Bi5HwGsCHFT7+IjAGOBrbmhM2LEwBVnQ/Mh6BFUiQDf/obpxYJZhiGcUhRp41kCTBLRE4WkXEExvMFMT8LgIvC4/OAezSomiwALghHdZ1MUDF/xDFOwzAMo43U1iJR1QERuQRYBPQA16rqChG5AuhV1QXANcANItJH0BK5IAy7QkRuAVYCA8CfqupBgKQ468qDYRiGkU9txvZuoqix3TAM41DG1dhuM9sNwzCMUpgiMQzDMEphisQwDMMohSkSwzAMoxSmSAzDMIxSHBKjtkRkM7C+YPCpwJYKxRkJWJ4PDQ61PB9q+YXyeX6Nqk7L83RIKJIyiEivy/C30YTl+dDgUMvzoZZfaF+erWvLMAzDKIUpEsMwDKMUpkjymd9pATqA5fnQ4FDL86GWX2hTns1GYhiGYZTCWiSGYRhGKUyRpCAic0VktYj0icilnZanKkTkRBG5V0SeFJEVIvKp0H2yiPxMRNaE/8eG7iIiXw/vwxMi8uudzUFxRKRHRH4pIreH5yeLyOIwzzeHWxMQbl9wc5jnxSIys5NyF0VEjhGRW0VkVfi83z7an7OI/K+wXC8XkRtF5PDR9pxF5FoR2RTuMNtw836uInJR6H+NiFyUlJYrpkgSEJEe4CrgbOA04EIROa2zUlXGAPDnqvqrwNuAPw3zdilwt6rOAu4OzyG4B7PC3zzgG+0XuTI+BTwZOf8icGWY523AxaH7xcA2VT0VuDL0NxL5GnCnqr4eeCNB3kftcxaR6cAngdmq+gaCrSYuYPQ95+8Ac2NuXs9VRCYDnyPY2nwO8LmG8imEqtov9gPeDiyKnF8GXNZpuWrK623AWcBq4ITQ7QRgdXh8NXBhxP+Qv5H0I9hN827gvcDtBNs5bwHGxJ85wX43bw+Px4T+pNN58MzvUcDTcblH83MGpgPPAZPD53Y78IHR+JyBmcDyos8VuBC4OuLe5M/3Zy2SZBoFskF/6DaqCJvybwIWA8er6gsA4f9xobfRci++CvwlMBieTwG2q+pAeB7N11Cew+s7Qv8jiVOAzcC3w+68b4nIEYzi56yqzwNfAZ4FXiB4bksZ3c+5ge9zrfR5myJJRhLcRtXwNhGZBPwQ+DNVfTnLa4LbiLoXIvJbwCZVXRp1TvCqDtdGCmOAXwe+oapvAnYx3N2RxIjPc9g1cy5wMvBq4AiCrp04o+k555GWx0rzbookmX7gxMj5DGBDh2SpHBEZS6BEvqeq/x46bxSRE8LrJwCbQvfRcC/eCZwjIs8ANxF0b30VOEZEGttNR/M1lOfw+tEEW0GPJPqBflVdHJ7fSqBYRvNz/k3gaVXdrKoHgH8H3sHofs4NfJ9rpc/bFEkyS4BZ4WiPcQQGuwUdlqkSRESAa4AnVfWfI5cWAI2RGxcR2E4a7h8LR3+8DdjRaEKPFFT1MlWdoaozCZ7lPar6EeBe4LzQWzzPjXtxXuh/RNVUVfVF4DkR+ZXQ6X3ASkbxcybo0nqbiEwMy3kjz6P2OUfwfa6LgPeLyLFhS+79oVsxOm006tYf8EHgKWAt8NedlqfCfL2LoAn7BPBY+PsgQd/w3cCa8H9y6F8IRrCtBZYRjIjpeD5K5P89wO3h8SnAI0Af8ANgfOh+eHjeF14/pdNyF8zrGUBv+Kx/DBw72p8z8HfAKmA5cAMwfrQ9Z+BGAhvQAYKWxcVFnivw+2He+4DfKyOTzWw3DMMwSmFdW4ZhGEYpTJEYhmEYpTBFYhiGYZTCFIlhGIZRClMkhmEYRilMkRhGBiJyUEQei/wyV4IWkU+IyMcqSPcZEZlaINwHRORvw/kBC8vKYRgujMn3YhiHNHtU9QxXz6r6b3UK48C7CSbgnQn8R4dlMQ4RTJEYRgHC5VZuBn4jdPpdVe0Tkb8FXlHVr4jIJ4FPECzdv1JVLwiX776WYJLcbmCeqj4hIlMIJppNI5gcJ5G0/gfB8ujjCBbY/BNVPRiT53yCVapPIVhv6njgZRF5q6qeU8c9MIwG1rVlGNlMiHVtnR+59rKqzgH+hWDtrjiXAm9S1f9EoFAgmHn9y9DtfwPXh+6fA36hwQKLC4CTAETkV4HzgXeGLaODwEfiCanqzQRraS1X1dMJZna/yZSI0Q6sRWIY2WR1bd0Y+b8y4foTwPdE5McES5RAsETNbwOo6j0iMkVEjiboivrvoftPRGRb6P99wJuBJcHyUUxgeEG+OLMIlsIAmKiqOx3yZxilMUViGMXRlOMG/4VAQZwDfFZEfo3s5buT4hDgOlW9LEsQEekFpgJjRGQlcIKIPAb8T1V9IDsbhlEO69oyjOKcH/l/KHpBRA4DTlTVewk21DoGmAT8nLBrSkTeA2zRYD+YqPvZBAssQrAA33kiclx4bbKIvCYuiKrOBn5CYB/5EsFCo2eYEjHagbVIDCObCWHNvsGdqtoYAjxeRBYTVMgujIXrAb4bdlsJwZ7h20Nj/LdF5AkCY3tj6e+/A24UkUeB+wmWREdVV4rI3wA/DZXTAeBPgfUJsv46gVH+T4B/TrhuGLVgq/8aRgHCUVuzVXVLp2UxjE5jXVuGYRhGKaxFYhiGYZTCWiSGYRhGKUyRGIZhGKUwRWIYhmGUwhSJYRiGUQpTJIZhGEYpTJEYhmEYpfj/4tKqQilmeBwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56b82594e0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I need to have the same actor to select actions\n",
    "# and the same replay buffer for both agents!\n",
    "\n",
    "def ddpg(n_episodes=1000, max_t=300):\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    total_scores = []\n",
    "    #max_score = -np.Inf\n",
    "    \n",
    "    noise = 10.0\n",
    "    #noise_reduction = 0.9999\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        state_0 = env_info.vector_observations[0]\n",
    "        state_1 = env_info.vector_observations[1]\n",
    "        \n",
    "        #se resetea el noise aqu√≠!\n",
    "        agent[0].reset()\n",
    "        agent[1].reset()\n",
    "        \n",
    "        scores = np.zeros(num_agents)\n",
    "        \n",
    "        #for t in range(max_t):\n",
    "        while True:\n",
    "            \n",
    "            #print(states.shape)\n",
    "            \n",
    "            action_0 = agent[0].act(state_0, noise, True)\n",
    "            action_1 = agent[1].act(state_1, noise, True)\n",
    "            \n",
    "            action = [[action_0[0], action_0[1]], [action_1[0], action_1[1]]]\n",
    "            \n",
    "            #noise *= noise_reduction\n",
    "            \n",
    "            #print(action)\n",
    "            \n",
    "            #print(actions.shape)\n",
    "            \n",
    "            env_info = env.step(action)[brain_name]\n",
    "            \n",
    "            #print(\"env_info shape: \", env_info.shape)\n",
    "            \n",
    "            next_state_0 = env_info.vector_observations[0]\n",
    "            next_state_1 = env_info.vector_observations[1]\n",
    "            \n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            #print(dones)\n",
    "            \n",
    "            agent[0].step(state_0, action_0, env_info.rewards[0], next_state_0, dones[0])\n",
    "            agent[1].step(state_1, action_1, env_info.rewards[1], next_state_1, dones[1])\n",
    "            \n",
    "            \n",
    "            state_0 = next_state_0\n",
    "            state_1 = next_state_1\n",
    "            \n",
    "            scores += env_info.rewards\n",
    "            \n",
    "            if np.any(dones): \n",
    "                break\n",
    "                \n",
    "        \n",
    "                \n",
    "        # scores is a list with total rewards for episode!\n",
    "        # I need to get the max of the two agents' scores!\n",
    "        max_score = np.amax(scores)\n",
    "        scores_deque.append(max_score)\n",
    "        \n",
    "        #print(scores_deque)\n",
    "        \n",
    "        total_scores.append(max_score)\n",
    "        \n",
    "        #print(scores)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), max_score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            torch.save(agent[0].actor_local.state_dict(), 'checkpoint_actor_0.pth')\n",
    "            torch.save(agent[0].critic_local.state_dict(), 'checkpoint_critic_0.pth')\n",
    "            torch.save(agent[1].actor_local.state_dict(), 'checkpoint_actor_1.pth')\n",
    "            torch.save(agent[1].critic_local.state_dict(), 'checkpoint_critic_1.pth')\n",
    "            \n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))   \n",
    "    return total_scores\n",
    "\n",
    "total_scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(total_scores)+1), total_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3460129   0.73822793]\n",
      " [-0.27360188  0.53664333]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.61334337 -0.04029378]\n",
      " [-0.21257078 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          0.10635757]\n",
      " [ 1.          0.26241414]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          1.        ]\n",
      " [ 1.          0.12505726]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -0.3345997 ]\n",
      " [-0.45515396  0.85046672]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          1.        ]\n",
      " [-0.33873783  0.13901067]]\n",
      "[0.0, 0.0]\n",
      "[[-0.47913703 -0.85849131]\n",
      " [ 1.         -0.74967009]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.50569408  0.47636157]\n",
      " [-0.10845614  0.16389899]]\n",
      "[0.0, 0.0]\n",
      "[[-0.536965   -1.        ]\n",
      " [-0.34598611  0.74403983]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -1.        ]\n",
      " [ 0.30673835 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.13586634]\n",
      " [-1.         -0.41765804]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -1.        ]\n",
      " [-0.69780219 -0.76670163]]\n",
      "[0.0, 0.0]\n",
      "[[-0.44845007  0.95796229]\n",
      " [-0.68710891 -0.2001238 ]]\n",
      "[-0.009999999776482582, 0.0]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "[[-1.         0.8556274]\n",
      " [-1.        -1.       ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.49637958  1.        ]\n",
      " [-0.05155019 -0.57305302]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -1.        ]\n",
      " [ 0.35272026 -0.81220978]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          1.        ]\n",
      " [ 0.01526596  0.90825684]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.42464798 -1.        ]\n",
      " [ 0.07790115 -0.36701604]]\n",
      "[0.0, 0.0]\n",
      "[[-0.56892185  1.        ]\n",
      " [-0.74778609 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          0.50135514]\n",
      " [-1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.08792961  0.02373645]\n",
      " [ 1.          0.35402811]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -0.56800344]\n",
      " [ 0.25271273 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.85555334 -0.08124946]\n",
      " [-0.87518879 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.40020545  0.16475322]\n",
      " [-0.59859132  0.80997734]]\n",
      "[0.0, 0.0]\n",
      "[[-0.2929788   1.        ]\n",
      " [-0.69787718 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.70158747  0.22488676]\n",
      " [ 0.59548202  0.38952011]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.61891873  1.        ]\n",
      " [-0.74141928 -0.50020381]]\n",
      "[-0.009999999776482582, 0.0]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "[[ 0.56209424 -0.07582128]\n",
      " [ 0.37051747  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.62600167  1.        ]\n",
      " [ 0.63294763 -0.01933658]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.23250619 -0.79282332]\n",
      " [ 0.10745378  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.51613646  1.        ]\n",
      " [-0.4698814   0.13418637]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.70442224  0.77030573]\n",
      " [-0.13997416 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.63999028  0.96793001]\n",
      " [ 0.53081677  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.04898346 -1.        ]\n",
      " [ 0.48613395  0.2527699 ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.93111209 -1.        ]\n",
      " [-0.04759215  0.57693214]]\n",
      "[0.0, 0.0]\n",
      "[[-0.14087088 -1.        ]\n",
      " [-0.59000118  0.54515626]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.93579235 -1.        ]\n",
      " [-0.65436333 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.57219634]\n",
      " [ 1.         -0.07081388]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          1.        ]\n",
      " [-1.          0.44501739]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -1.        ]\n",
      " [-0.07875176  0.6028602 ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.94588643 -0.67463355]\n",
      " [-0.9814455  -0.41080711]]\n",
      "[0.0, 0.0]\n",
      "[[-0.46786289 -0.82226397]\n",
      " [-1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.98900443 -0.80447503]\n",
      " [ 1.         -0.4584121 ]]\n",
      "[0.0, -0.009999999776482582]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "[[ 0.04104607  1.        ]\n",
      " [ 0.10599426  0.44052975]]\n",
      "[0.0, 0.0]\n",
      "[[-0.29749302  0.26130982]\n",
      " [ 1.          0.54364149]]\n",
      "[0.0, 0.0]\n",
      "[[-0.4927614   1.        ]\n",
      " [ 0.35642327  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.37293138 -0.60369326]\n",
      " [-0.19155962  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.06308646  0.22205296]\n",
      " [ 1.          1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.56473729 -0.68815412]\n",
      " [ 0.63736637  0.04939977]]\n",
      "[0.0, 0.0]\n",
      "[[-0.39717224  0.07699112]\n",
      " [ 1.          1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.61179502 -0.81721074]\n",
      " [-0.06578817  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.64445905]\n",
      " [-0.63947063  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          0.54167426]\n",
      " [-0.38927912  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.31344018 -0.50990251]\n",
      " [ 1.         -0.41268912]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -0.2768923 ]\n",
      " [-0.15373057 -0.43275594]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.60386023  0.81182143]\n",
      " [ 0.55276996 -0.86587256]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.52805894]\n",
      " [-0.83023814 -0.28955465]]\n",
      "[-0.009999999776482582, 0.0]\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "[[-0.72946414 -0.64355365]\n",
      " [ 0.57708047 -0.93104458]]\n",
      "[0.0, 0.0]\n",
      "[[-0.06362041 -0.86048707]\n",
      " [-0.99626785  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.46336661  1.        ]\n",
      " [ 1.          1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.85969741  0.99646528]\n",
      " [-0.87942163 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.81256609 -0.71346982]\n",
      " [-0.64367736 -0.28552645]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -0.72930371]\n",
      " [ 1.          1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          1.        ]\n",
      " [ 0.04608861 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -0.90066198]\n",
      " [-0.74450655  0.74986599]]\n",
      "[0.0, 0.0]\n",
      "[[-0.06763788  1.        ]\n",
      " [ 0.4878204  -0.77580937]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.47396991]\n",
      " [ 0.5410415   0.65606249]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.07637388  1.        ]\n",
      " [ 0.7890383  -0.41449324]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -1.        ]\n",
      " [-0.83120305  0.08517959]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.50721419 -1.        ]\n",
      " [ 1.         -0.26868634]]\n",
      "[0.0, 0.0]\n",
      "[[-0.17829848 -0.7879511 ]\n",
      " [ 1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.46992936 -0.48909602]\n",
      " [-1.          0.35648738]]\n",
      "[0.0, 0.0]\n",
      "[[-0.15830138 -0.23682333]\n",
      " [-0.93144618 -0.87480573]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.39079984  0.05976372]\n",
      " [-0.23954116 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.98081121  1.        ]\n",
      " [ 0.18842328  1.        ]]\n",
      "[0.0, 0.10000000149011612]\n",
      "[[-1.         -0.57205298]\n",
      " [-1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.08015398 -0.81475591]\n",
      " [-1.          1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.06253768 -0.6618984 ]\n",
      " [ 0.06424651  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.34778123 -1.        ]\n",
      " [ 0.71269442  0.67881928]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.50464869]\n",
      " [ 1.         -0.12158068]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.         -1.        ]\n",
      " [-0.44485009  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-0.13653065 -0.94471601]\n",
      " [ 1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[-1.          0.39967588]\n",
      " [-0.30940818 -0.50563245]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.          0.53450396]\n",
      " [-0.22258711  1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.21101961  0.10833869]\n",
      " [-1.         -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 0.30402778 -0.18965309]\n",
      " [ 1.         -0.65123347]]\n",
      "[0.0, 0.0]\n",
      "[[-1.         -0.3622307 ]\n",
      " [-0.18891186 -1.        ]]\n",
      "[0.0, 0.0]\n",
      "[[ 1.  1.]\n",
      " [-1.  1.]]\n",
      "[0.0, -0.009999999776482582]\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    \n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        \n",
    "        print(actions)\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        \n",
    "        \n",
    "        \n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        \n",
    "        print(rewards)\n",
    "        \n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
